<div align="center">

<h1>Beyond the Last Frame: Process-aware Evaluation for <br> Generative Video Reasoning</h1>

<div align="center">
  <a href='https://arxiv.org/abs/2512.24952'><img src='https://img.shields.io/badge/Arxiv-2512.24952-red'></a>
  <a href='https://huggingface.co/datasets/Monosail/VIPER'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Benchmark-yellow'></a>
</div>

<br>

<p align="center">
  <img src="assets/outcome_hack.png" width="85%"> <br>
  <em>Illustration of outcome-hacking, where the generated video has the correct final state but an incorrect process.</em>
</p>

</div>

---

## üëÄ Overview

Current video generation models often suffer from **Outcome-hacking**: they may generate a video with the correct final outcome but a wrong process. This hacks traditional single-frame evaluation metrics.

**VIPER** (VIdeo Process Evaluation for Reasoning) is designed to bridge this gap:

* **üèÜ Comprehensive Benchmark:** 309 carefully curated samples spanning **6 distinct domains** (Temporal, Structural, Symbolic, Spatial, Physics, and Planning).
* **üìè New Metric (POC@r):** **P**rocess-**O**utcome **C**onsistency. We evaluate correctness at both the process and outcome levels by uniformly sampling frames at rate $r$.
* **üö´ Failure Pattern:** We identify and summarize four common failure patterns in current generative video models.

---
<div>
<p align="center">
  <img src="assets/overview.png" width="85%"> <br>
  <em>Overview of VIPER. VIPER consists of 16 tasks from 6 domains</em>
</p>
</div>

## üìä Dataset Statistics

VIPER covers diverse reasoning tasks to ensure a holistic evaluation of video generation capabilities.

| Domain | Samples | Task Types |
| :--- | :---: | :--- |
| **Physics** | 32 | experiment, game |
| **Planning** | 44 | navigation, manipulation |
| **Spatial** | 60 | rotate, restore |
| **Structural** | 70 | chess, maze, sudoku |
| **Symbolic** | 60 | math, multimodal |
| **Temporal** | 43 | obj_move, zoom |

---

## üöÄ Quick Start

### Download
```Python
from datasets import load_dataset

# Load the full VIPER benchmark
dataset = load_dataset("Monosail/VIPER")
```

### Data Fields

- `id`: Unique identifier for the sample
- `domain`: The reasoning domain (Physics, Planning, Spatial, Structural, Symbolic, Temporal)
- `task_type`: Specific task category within the domain
- `prompt`: Text prompt describing the task
- `image`: The input image
- `reference_frames`: Ground-truth image frames
- `reference_texts`: Ground-truth text descriptions
- `protocol`: Process-level task constraints

## üõ†Ô∏è Evaluation

The evaluation pipeline is split into two stages: **inference** and **judgement**.
During **inference**, we provide scripts to generate inference outputs on the **VIPER** datasets using the following supported models:
- **Closed-source (API)**
  - Sora2  
  - Veo3.1  
  - Seedance 1.5 Pro (Opened)  
  - Wan2.6 (Opened)  
- **Open-source**
  - Wan2.2  
  - Hunyuan-1.5  
During **judgement**, we use the **OpenRouter API** and default to **gpt-5**. You may use any MLLM as long as it is compatible with the provider endpoint.

### Inference

#### Seedance 1.5 pro

To run video inference with Seedance 1.5 Pro:
```bash
bash scripts/run_sd.sh
```
Prerequisites:

- Apply for the [Seedance API](https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-seedance-1-5-pro)
- Set the environment variable `ARK_API_KEY`

#### Wan2.6

To run video inference with Wan2.6:

```bash
bash scripts/run_wan26.sh
```
Prerequisites:

- Apply for the [Wan2.6 api](https://bailian.console.aliyun.com/cn-beijing/#/home)
- Set the environment variable `DASHSCOPE_API_KEY`

### Judgement

The whole judgement consists of vlm-as-a-judge which calls external vlm for correctness and scoring which summaries the original returns into pass@k acc table, which are both covered in one script. 

A sample command to run judgement on seedance model is provided below. Check the script for more info including running on single file, controlling the precise behavior or resume from former files.
```bash
eval/scripts/eval.sh \
  --data_path ./results/video_inference/test_doubao-seedance-1-5-pro-251215 \
  --output_path ./results/vlm_judge/test_doubao-seedance-1-5-pro-251215 \
  --fps 1.0 \
  --model_name gpt-5-chat-2025-08-07 \
  --pass_k 1 \
  --max_workers 8
```

Notes:
- In our experiment, we use [OpenRouter](https://openrouter.ai/) as our API vendor to maximize compatibility. If you have access, put `OPENROUTER_API_KEY` in the `.env` file.
- As far as we know, the OpenRouter API is compatible with official implementations. You can check and implement your API vendor in `eval/scripts/gpt-4o.py`.


## üìù Citation

If you find our benchmark useful for your research, please consider citing:

```bibtex
@article{li2026viper,
  title={Beyond the Last Frame: Process-aware Evaluation for Generative Video Reasoning},
  author={Li, Yifan and Gu, Yukai and Min, Yingqian and Liu, Zikang and Du, Yifan and Zhou, Kun and Yang, Min and Zhao, Wayne Xin and Qiu, Minghui},
  journal={arXiv preprint arXiv:2512.24952},
  year={2025}
}
```

